# -*- coding: utf-8 -*-
"""pizabay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d99xtE54Q-FLrjnVnCnSqSxhygBhxW_4
"""

!pip install requests tqdm autodistill autodistill-grounding-dino autodistill-yolov8 autodistill-segment-anything ultralytics roboflow

# -----------------------------
# Install required packages
# -----------------------------
!pip install ultralytics --upgrade
!pip install autodistill_grounding_dino
!pip install opencv-python-headless
!pip install pyyaml
!pip install requests
!pip install roboflow
# -----------------------------
# Imports
# -----------------------------
import os
import shutil
import requests
import urllib
import random
import yaml
from autodistill_grounding_dino import GroundingDINO
from autodistill.detection import CaptionOntology

# -----------------------------
# User Configurations
# -----------------------------
API_KEY = ""  # <--- Replace this
PIXABAY_QUERIES = ["cows in farm"]  # search queries
LABELS = ["cow"]  # YOLO class names
IMG_DIR = "./pixabay_images"  # folder to save downloaded images
SAVE_DIR = "./pixabay_dataset"  # final dataset folder
os.makedirs(IMG_DIR, exist_ok=True)
os.makedirs(SAVE_DIR, exist_ok=True)

# -----------------------------
# Helper Functions
# -----------------------------
def download_pixabay_images(query, per_page=200, max_pages=5):
    count = 0
    for page in range(1, max_pages + 1):
        url = f"https://pixabay.com/api/?key={API_KEY}&q={urllib.parse.quote(query)}&image_type=photo&per_page={per_page}&page={page}"
        try:
            resp = requests.get(url, timeout=10)
            resp.raise_for_status()
            resp = resp.json()
        except Exception as e:
            print(f"Failed to fetch page {page} for query '{query}': {e}")
            break

        hits = resp.get("hits", [])
        if not hits:
            print(f"No results on page {page} for query '{query}'")
            break

        for i, hit in enumerate(hits):
            try:
                img_data = requests.get(hit["largeImageURL"], timeout=10).content
                filename = os.path.join(IMG_DIR, f"{query.replace(' ','_')}_{page}_{i}.jpg")
                with open(filename, "wb") as f:
                    f.write(img_data)
                count += 1
            except Exception as e:
                print(f"Failed to download image {i} on page {page}: {e}")
                continue
    print(f"Downloaded {count} images for '{query}'")
    return count

# -----------------------------
# Download Images
# -----------------------------
for q in PIXABAY_QUERIES:
    download_pixabay_images(q)

# -----------------------------
# Auto-label Images using GroundingDINO
# -----------------------------
print("ðŸ·ï¸ Auto-labeling with GroundingDINO...")
base_model = GroundingDINO(ontology=CaptionOntology({label: label for label in LABELS}))
base_model.label(IMG_DIR)
print("âœ… Labeling done. Labeled images saved in:", IMG_DIR + "_labeled")

# -----------------------------
# Split dataset into train/val
# -----------------------------
TRAIN_DIR = os.path.join(SAVE_DIR, "train")
VAL_DIR = os.path.join(SAVE_DIR, "val")
for d in [TRAIN_DIR, VAL_DIR]:
    os.makedirs(os.path.join(d, "images"), exist_ok=True)
    os.makedirs(os.path.join(d, "labels"), exist_ok=True)

labeled_img_dir = IMG_DIR + "_labeled/train/images"
labeled_label_dir = IMG_DIR + "_labeled/train/labels"
images = [f for f in os.listdir(labeled_img_dir) if f.endswith(".jpg")]
random.shuffle(images)
split = int(0.8 * len(images))
train_imgs = images[:split]
val_imgs = images[split:]

for img_list, dest in [(train_imgs, TRAIN_DIR), (val_imgs, VAL_DIR)]:
    for img in img_list:
        shutil.copy(os.path.join(labeled_img_dir, img), os.path.join(dest, "images", img))
        label_file = os.path.splitext(img)[0] + ".txt"
        src_label = os.path.join(labeled_label_dir, label_file)
        dst_label = os.path.join(dest, "labels", label_file)
        if os.path.exists(src_label):
            shutil.copy(src_label, dst_label)

print("âœ… Dataset split into train/val")

# -----------------------------
# Create data.yaml for YOLOv8
# -----------------------------
data_yaml_content = f"""
train: ../pixabay_images_labeled/train/images
val: ../pixabay_images_labeled/valid/images

nc: {len(LABELS)}
names: {LABELS}
"""

yaml_path = os.path.join(SAVE_DIR, "data.yaml")
print(yaml_path)
with open(yaml_path, "w") as f:
    f.write(data_yaml_content)
print("âœ… data.yaml created at", yaml_path)

'''
yaml_content = """
train: ../pixabay_images_labeled/train/images
val: ../pixabay_images_labeled/valid/images

nc: 1
names: ["cow"]
"""

with open("pixabay_dataset/data.yaml", "w") as f:
    f.write(yaml_content)

print("âœ… data.yaml fixed and saved!")
'''

from ultralytics import YOLO

model = YOLO("best.pt")

model.train(
    data="pixabay_dataset/data.yaml",
    epochs=10,
    imgsz=640,
    batch=16
)

from ultralytics import YOLO

# Load the fine-tuned model (replace 'runs/detect/train4/weights/best.pt' with the actual path if different)
# You can find the exact path in the output of the training cell (cell 6N-QlIt6QenY)
model = YOLO("/content/runs/detect/train4/weights/best.pt") # give path of the model to be converted

# Export the model to ONNX format
model.export(format="onnx")

print("âœ… Model successfully exported to ONNX format!")

from ultralytics import YOLO
import os

# -----------------------------
# Test the trained model
# -----------------------------

# Path to the best weights from your training run
# This path is relative to the directory where your notebook is running
best_weights_path = "/content/runs/detect/train/weights/best.pt"

# Check if the weights file exists
if not os.path.exists(best_weights_path):
    print(f"Error: Best weights file not found at {best_weights_path}")
else:
    # Load the trained model
    model = YOLO(best_weights_path)

    # Path to an example image for testing
    # You can change this to the path of your test image
    example_image_path = "/content/pixabay_images_labeled/valid/images/cows_in_farm_1_10.jpg" # Example image from your validation set

    # Check if the example image exists
    if not os.path.exists(example_image_path):
        print(f"Error: Example image not found at {example_image_path}")
    else:
        # Run inference on the example image
        results = model.predict(example_image_path)

        # Display the results (this will typically show the image with bounding boxes)
        for r in results:
            r.show()
